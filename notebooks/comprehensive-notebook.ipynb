{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Environment Setup\n",
    "\n",
    "This notebook is part of a workshop demonstrating pgvector with Amazon Aurora PostgreSQL and Amazon Bedrock. We'll automatically configure your environment using AWS Secrets Manager and environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "psycopg[binary]==3.1.16\n",
    "pandas>=2.0.0\n",
    "numpy>=1.24.0\n",
    "boto3>=1.34.0\n",
    "tqdm>=4.66.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required Python packages\"\"\"\n",
    "    print(\"Installing required packages...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "        print(\"\\nâœ… Package installation complete\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing packages: {e}\")\n",
    "        raise\n",
    "\n",
    "# Install packages if needed\n",
    "try:\n",
    "    import psycopg\n",
    "    print(\"psycopg3 is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing required packages including psycopg3...\")\n",
    "    install_packages()\n",
    "\n",
    "# Import required packages\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "def setup_workshop_environment():\n",
    "    \"\"\"Set up workshop environment variables using AWS Secrets Manager\"\"\"\n",
    "    try:\n",
    "        # Get AWS region from instance metadata\n",
    "        region = boto3.session.Session().region_name\n",
    "        if not region:\n",
    "            print(\"âš ï¸ Could not determine AWS region. Using default region us-west-2\")\n",
    "            region = 'us-west-2'\n",
    "        \n",
    "        print(f\"Using AWS region: {region}\")\n",
    "        \n",
    "        # Initialize AWS clients\n",
    "        secrets_client = boto3.client('secretsmanager', region_name=region)\n",
    "        rds_client = boto3.client('rds', region_name=region)\n",
    "        \n",
    "        # Get database credentials from Secrets Manager\n",
    "        secret_name = \"apg-pgvector-secret-RIV\"\n",
    "        try:\n",
    "            secret_response = secrets_client.get_secret_value(SecretId=secret_name)\n",
    "            db_credentials = json.loads(secret_response['SecretString'])\n",
    "            \n",
    "            # Set environment variables\n",
    "            os.environ['PGUSER'] = db_credentials.get('username')\n",
    "            os.environ['PGPASSWORD'] = db_credentials.get('password')\n",
    "            os.environ['PGDATABASE'] = 'postgres'\n",
    "            \n",
    "            print(\"âœ… Database credentials retrieved from Secrets Manager\")\n",
    "            \n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            print(f\"âš ï¸ Error accessing Secrets Manager: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Get cluster endpoint\n",
    "        cluster_id = \"apg-pgvector-riv\"\n",
    "        try:\n",
    "            response = rds_client.describe_db_clusters(\n",
    "                DBClusterIdentifier=cluster_id\n",
    "            )\n",
    "            \n",
    "            if response['DBClusters']:\n",
    "                cluster = response['DBClusters'][0]\n",
    "                os.environ['PGHOST'] = cluster['Endpoint']\n",
    "                os.environ['PGPORT'] = str(cluster['Port'])\n",
    "                print(\"âœ… Database endpoint information retrieved\")\n",
    "            \n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            print(f\"âš ï¸ Error accessing RDS: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Verify environment variables\n",
    "        required_vars = ['PGHOST', 'PGPORT', 'PGUSER', 'PGPASSWORD', 'PGDATABASE']\n",
    "        missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "        \n",
    "        if missing_vars:\n",
    "            print(\"\\nâš ï¸ Missing required environment variables:\")\n",
    "            print(\"\\n\".join(missing_vars))\n",
    "            return False\n",
    "        \n",
    "        print(\"\\nâœ… Workshop environment successfully configured\")\n",
    "        print(f\"\\nDatabase Host: {os.getenv('PGHOST')}\")\n",
    "        print(f\"Database Port: {os.getenv('PGPORT')}\")\n",
    "        print(f\"Database Name: {os.getenv('PGDATABASE')}\")\n",
    "        print(f\"Database User: {os.getenv('PGUSER')}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸ Error setting up workshop environment: {str(e)}\")\n",
    "        print(\"\\nIf you're having issues, please raise your hand for workshop support.\")\n",
    "        return False\n",
    "\n",
    "# Set up the workshop environment\n",
    "if setup_workshop_environment():\n",
    "    print(\"\\nðŸš€ You're ready to proceed with the workshop!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Environment setup failed. Please seek assistance from the workshop staff.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Package Installation\n",
    "\n",
    "Let's verify that all required packages are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all required packages\n",
    "try:\n",
    "    import psycopg\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import boto3\n",
    "    import tqdm\n",
    "\n",
    "    print(\"Installed package versions:\")\n",
    "    print(f\"psycopg: {psycopg.__version__}\")\n",
    "    print(f\"pandas: {pd.__version__}\")\n",
    "    print(f\"numpy: {np.__version__}\")\n",
    "    print(f\"boto3: {boto3.__version__}\")\n",
    "    print(f\"tqdm: {tqdm.__version__}\")\n",
    "    print(\"\\nâœ… All required packages are installed correctly\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Error importing packages: {e}\")\n",
    "    print(\"Please seek assistance from the workshop staff\")"
 ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Catalog Setup and Embedding Generation with pgvector and Amazon Bedrock\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Set up pgvector extension in Aurora PostgreSQL\n",
    "2. Create and configure a product catalog table with vector storage capabilities\n",
    "3. Load product data from a compressed CSV file\n",
    "4. Generate embeddings using Amazon Bedrock's Titan Text Embeddings model\n",
    "5. Store and index the embeddings for vector similarity search\n",
    "\n",
    "## Prerequisites\n",
    "- Aurora PostgreSQL cluster with pgvector extension available\n",
    "- AWS credentials configured with access to Amazon Bedrock\n",
    "- Product catalog data in CSV format (compressed with gzip)\n",
    "- Required Python packages: psycopg[binary], boto3, pandas, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "from psycopg.adapt import Dumper, PyFormat\n",
    "import gzip\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure numpy array adaptation for PostgreSQL\n",
    "class NumpyArrayDumper(Dumper):\n",
    "    def dump(self, obj):\n",
    "        return str(obj.tolist())\n",
    "\n",
    "psycopg.adapters.register_dumper(np.ndarray, NumpyArrayDumper)\n",
    "\n",
    "# Database connection parameters (from environment variables)\n",
    "DB_PARAMS = {\n",
    "    'dbname': os.getenv('PGDATABASE', 'postgres'),\n",
    "    'user': os.getenv('PGUSER'),\n",
    "    'password': os.getenv('PGPASSWORD'),\n",
    "    'host': os.getenv('PGHOST'),\n",
    "    'port': os.getenv('PGPORT', '5432')\n",
    "}\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostgreSQL Setup Functions\n",
    "\n",
    "First, we'll define functions to handle our PostgreSQL operations using psycopg3. Key differences from psycopg2 include:\n",
    "1. Context manager support for connections and cursors\n",
    "2. Native support for async/await (though we'll use sync here)\n",
    "3. Improved type adaptation system\n",
    "4. Better error handling and diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection():\n",
    "    \"\"\"Create and return a PostgreSQL database connection using psycopg3\"\"\"\n",
    "    try:\n",
    "        conn = psycopg.connect(\n",
    "            **DB_PARAMS,\n",
    "            autocommit=True,\n",
    "            row_factory=dict_row  # Return rows as dictionaries\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Set up the database with required extension and schema\"\"\"\n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create vector extension if it doesn't exist\n",
    "            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "            print(\"âœ… Vector extension created/verified\")\n",
    "            \n",
    "            # Create schema if it doesn't exist\n",
    "            cur.execute(\"CREATE SCHEMA IF NOT EXISTS bedrock_integration;\")\n",
    "            print(\"âœ… Schema created/verified\")\n",
    "            \n",
    "            # Create product catalog table with vector support\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS bedrock_integration.product_catalog (\n",
    "                    \"productId\" VARCHAR(255),\n",
    "                    product_description TEXT,\n",
    "                    imgUrl TEXT,\n",
    "                    productURL TEXT,\n",
    "                    stars NUMERIC,\n",
    "                    reviews INT,\n",
    "                    price NUMERIC,\n",
    "                    category_id INT,\n",
    "                    isBestSeller BOOLEAN,\n",
    "                    boughtInLastMonth INT,\n",
    "                    category_name VARCHAR(255),\n",
    "                    quantity INT,\n",
    "                    embedding vector(1024)\n",
    "                );\n",
    "            \"\"\")\n",
    "            print(\"âœ… Product catalog table created/verified\")\n",
    "            \n",
    "            # Create HNSW index for vector similarity search\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS product_catalog_embedding_idx \n",
    "                ON bedrock_integration.product_catalog \n",
    "                USING hnsw (embedding vector_cosine_ops);\n",
    "            \"\"\")\n",
    "            print(\"âœ… HNSW index created/verified\")\n",
    "\n",
    "# Execute setup\n",
    "setup_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Product Catalog Data\n",
    "\n",
    "Using psycopg3's improved batch execution capabilities for data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_product_data(file_path='datasets/product_catalog.csv.gz', chunk_size=1000):\n",
    "    \"\"\"Load product catalog data from gzipped CSV file into PostgreSQL\"\"\"\n",
    "    \n",
    "    # First, check if we already have data\n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM bedrock_integration.product_catalog;\")\n",
    "            count = cur.fetchone()['count']\n",
    "            if count > 0:\n",
    "                print(f\"Table already contains {count} rows. Skipping data load.\")\n",
    "                return\n",
    "\n",
    "    # Read the gzipped CSV file in chunks\n",
    "    chunks = pd.read_csv(\n",
    "        file_path, \n",
    "        compression='gzip',\n",
    "        chunksize=chunk_size\n",
    "    )\n",
    "    \n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            total_rows = 0\n",
    "            for chunk in tqdm(chunks, desc=\"Loading data chunks\"):\n",
    "                # Prepare values for insertion\n",
    "                values = [\n",
    "                    tuple(row) for row in chunk.replace({np.nan: None}).values\n",
    "                ]\n",
    "                \n",
    "                # Using psycopg3's execute_batch for better performance\n",
    "                psycopg.execute_batch(cur, \"\"\"\n",
    "                    INSERT INTO bedrock_integration.product_catalog (\n",
    "                        \"productId\", product_description, imgUrl, productURL,\n",
    "                        stars, reviews, price, category_id, isBestSeller,\n",
    "                        boughtInLastMonth, category_name, quantity\n",
    "                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "                \"\", values, page_size=100)\n",
    "                \n",
    "                total_rows += len(values)\n",
    "            \n",
    "            print(f\"âœ… Successfully loaded {total_rows} products\")\n",
    "\n",
    "# Execute data load\n",
    "load_product_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings with Amazon Bedrock\n",
    "\n",
    "Using psycopg3's improved connection handling for embedding generation and storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Generate embedding for a single text using Amazon Bedrock\"\"\"\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "            modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=json.dumps({\"inputText\": text})\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return np.array(response_body['embedding'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text: {text[:50]}...\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_products_batch(products, max_workers=10):\n",
    "    \"\"\"Process a batch of products to generate and store embeddings\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        embeddings = list(tqdm(\n",
    "            executor.map(get_embedding, products['product_description']),\n",
    "            total=len(products),\n",
    "            desc=\"Generating embeddings\"\n",
    "        ))\n",
    "    \n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Using psycopg3's execute_batch for better performance\n",
    "            updates = [\n",
    "                (embedding, product_id)\n",
    "                for embedding, product_id in zip(embeddings, products['productId'])\n",
    "                if embedding is not None\n",
    "            ]\n",
    "            \n",
    "            psycopg.execute_batch(cur, \"\"\"\n",
    "                UPDATE bedrock_integration.product_catalog\n",
    "                SET embedding = %s\n",
    "                WHERE \"productId\" = %s;\n",
    "            \"\", updates, page_size=100)\n",
    "    \n",
    "    return len([e for e in embeddings if e is not None])\n",
    "\n",
    "def generate_all_embeddings(batch_size=100):\n",
    "    \"\"\"Generate embeddings for all products without existing embeddings\"\"\"\n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Get products without embeddings\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT \"productId\", product_description\n",
    "                FROM bedrock_integration.product_catalog\n",
    "                WHERE embedding IS NULL;\n",
    "            \"\"\")\n",
    "            products = pd.DataFrame(cur.fetchall())\n",
    "            \n",
    "            if len(products) == 0:\n",
    "                print(\"No products need embeddings generated\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Generating embeddings for {len(products)} products\")\n",
    "            \n",
    "            # Process in batches\n",
    "            total_processed = 0\n",
    "            for i in range(0, len(products), batch_size):\n",
    "                batch = products.iloc[i:i+batch_size]\n",
    "                processed = process_products_batch(batch)\n",
    "                total_processed += processed\n",
    "                print(f\"Batch {i//batch_size + 1}: Processed {processed} products\")\n",
    "            \n",
    "            print(f\"âœ… Successfully generated embeddings for {total_processed} products\")\n",
    "\n",
    "# Generate embeddings for all products\n",
    "generate_all_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Setup\n",
    "\n",
    "Using psycopg3's improved row factory for better data access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_setup():\n",
    "    \"\"\"Verify the database setup and embedding generation\"\"\"\n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Check total products\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT COUNT(*)\n",
    "                FROM bedrock_integration.product_catalog;\n",
    "            \"\"\")\n",
    "            total_products = cur.fetchone()['count']\n",
    "            print(f\"Total products in catalog: {total_products}\")\n",
    "            \n",
    "            # Check products with embeddings\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT COUNT(*)\n",
    "                FROM bedrock_integration.product_catalog\n",
    "                WHERE embedding IS NOT NULL;\n",
    "            \"\"\")\n",
    "            products_with_embeddings = cur.fetchone()['count']\n",
    "            print(f\"Products with embeddings: {products_with_embeddings}\")\n",
    "            \n",
    "            # Check embedding dimensionality\n",
    "            if products_with_embeddings > 0:\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT array_length(embedding, 1)\n",
    "                    FROM bedrock_integration.product_catalog\n",
    "                    WHERE embedding IS NOT NULL\n",
    "                    LIMIT 1;\n",
    "                \"\"\")\n",
    "                embedding_dim = cur.fetchone()['array_length']\n",
    "                print(f\"Embedding dimensionality: {embedding_dim}\")\n",
    "            \n",
    "            # Check HNSW index\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT indexname, indexdef\n",
    "                FROM pg_indexes\n",
    "                WHERE tablename = 'product_catalog'\n",
    "                AND indexname = 'product_catalog_embedding_idx';\n",
    "            \"\"\")\n",
    "            index_info = cur.fetchone()\n",
    "            if index_info:\n",
    "                print(\"âœ… HNSW index is properly configured\")\n",
    "            else:\n",
    "                print(\"âš ï¸ HNSW index is missing\")\n",
    "            \n",
    "            # Calculate completion percentage\n",
    "            if total_products > 0:\n",
    "                completion_pct = (products_with_embeddings / total_products) * 100\n",
    "                print(f\"\\nEmbedding generation progress: {completion_pct:.1f}%\")\n",
    "            \n",
    "            # Sample query to verify vector similarity search\n",
    "            if products_with_embeddings > 0:\n",
    "                print(\"\\nTesting vector similarity search...\")\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \"productId\", product_description, stars\n",
    "                    FROM bedrock_integration.product_catalog\n",
    "                    WHERE embedding IS NOT NULL\n",
    "                    ORDER BY RANDOM()\n",
    "                    LIMIT 1;\n",
    "                \"\"\")\n",
    "                sample_product = cur.fetchone()\n",
    "                \n",
    "                if sample_product:\n",
    "                    print(f\"\\nSample product: {sample_product['product_description'][:100]}...\")\n",
    "                    \n",
    "                    # Get embedding for sample product\n",
    "                    cur.execute(\"\"\"\n",
    "                        SELECT embedding\n",
    "                        FROM bedrock_integration.product_catalog\n",
    "                        WHERE \"productId\" = %s;\n",
    "                    \"\", (sample_product['productId'],))\n",
    "                    sample_embedding = cur.fetchone()['embedding']\n",
    "                    \n",
    "                    # Find similar products using pgvector's cosine similarity operator\n",
    "                    cur.execute(\"\"\"\n",
    "                        SELECT \"productId\", product_description,\n",
    "                               1 - (embedding <=> %s) as similarity\n",
    "                        FROM bedrock_integration.product_catalog\n",
    "                        WHERE \"productId\" != %s\n",
    "                        ORDER BY embedding <=> %s\n",
    "                        LIMIT 3;\n",
    "                    \"\", (sample_embedding, sample_product['productId'], sample_embedding))\n",
    "                    \n",
    "                    similar_products = cur.fetchall()\n",
    "                    print(\"\\nSimilar products:\")\n",
    "                    for prod in similar_products:\n",
    "                        print(f\"Similarity: {prod['similarity']:.3f}\")\n",
    "                        print(f\"Description: {prod['product_description'][:100]}...\\n\")\n",
    "\n",
    "# Run verification\n",
    "verify_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sample Semantic Search Function\n",
    "\n",
    "Let's add a utility function to perform semantic search using natural language queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query_text, limit=5):\n",
    "    \"\"\"Search for products using semantic similarity to the query text\"\"\"\n",
    "    # Generate embedding for the query text\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    if query_embedding is None:\n",
    "        print(\"Failed to generate embedding for query\")\n",
    "        return\n",
    "    \n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Search using vector similarity\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT \n",
    "                    \"productId\",\n",
    "                    product_description,\n",
    "                    stars,\n",
    "                    price,\n",
    "                    category_name,\n",
    "                    1 - (embedding <=> %s) as similarity\n",
    "                FROM bedrock_integration.product_catalog\n",
    "                WHERE embedding IS NOT NULL\n",
    "                ORDER BY embedding <=> %s\n",
    "                LIMIT %s;\n",
    "            \"\", (query_embedding, query_embedding, limit))\n",
    "            \n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            if not results:\n",
    "                print(\"No matching products found\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Top {len(results)} matches for: '{query_text}'\\n\")\n",
    "            for result in results:\n",
    "                print(f\"Similarity: {result['similarity']:.3f}\")\n",
    "                print(f\"Category: {result['category_name']}\")\n",
    "                print(f\"Price: ${result['price']:.2f}\")\n",
    "                print(f\"Stars: {result['stars']}\")\n",
    "                print(f\"Description: {result['product_description'][:200]}...\\n\")\n",
    "\n",
    "# Example usage\n",
    "print(\"Example semantic search queries:\")\n",
    "queries = [\n",
    "    \"comfortable running shoes for marathon training\",\n",
    "    \"stylish waterproof backpack for college\",\n",
    "    \"professional chef knife set with wooden block\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    semantic_search(query, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "1. Set up pgvector extension in our Aurora PostgreSQL database\n",
    "2. Created a product catalog table with vector storage capabilities\n",
    "3. Loaded product data from a compressed CSV file\n",
    "4. Generated embeddings using Amazon Bedrock's Titan Text Embeddings model\n",
    "5. Stored and indexed the embeddings for efficient vector similarity search\n",
    "6. Implemented semantic search functionality using natural language queries\n",
    "\n",
    "The system demonstrates several advanced features:\n",
    "- Efficient data loading with chunked processing\n",
    "- Parallel embedding generation with thread pooling\n",
    "- HNSW indexing for fast approximate nearest neighbor search\n",
    "- Vector similarity search using pgvector's operators\n",
    "- Natural language semantic search capabilities\n",
    "\n",
    "For production use, consider:\n",
    "- Adding error handling and retry logic for embedding generation\n",
    "- Implementing batch processing for large datasets\n",
    "- Monitoring embedding generation performance\n",
    "- Setting up regular data updates and embedding refresh processes\n",
    "- Implementing caching for frequently accessed embeddings\n",
    "- Adding input validation and sanitization for search queries\n",
    "\n",
    "The combination of pgvector and Amazon Bedrock provides a powerful foundation for building semantic search and recommendation systems that understand natural language queries and find relevant products based on semantic similarity rather than just keyword matching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}